# SPDX-License-Identifier: Apache-2.0

# bitsandbytes
click>=8.1.7,<9.0.0
click-didyoumean>=0.3.0,<0.4.0
datasets>=2.18.0,<3.0.0
gguf>=0.6.0,<0.7.0
GitPython>=3.1.42,<4.0.0
httpx
jsonschema>=4.21.1,<5.0.0
langchain-text-splitters
# pin version, lift restriction after testing >=0.2.58
# see https://github.com/abetlen/llama-cpp-python/issues/1286
llama_cpp_python==0.2.55
mlx>=0.5.1,<0.6.0
numpy>=1.26.4,<2.0.0
openai>=1.13.3,<2.0.0
peft>=0.9.0,<0.10.0
pre-commit>=3.0.4,<4.0
prompt-toolkit>=3.0.38,<4.0.0
pydantic
pydantic_yaml
pydeps>=1.12.12,<2
pylint>=2.16.2,<4.0
pytest
pytest-asyncio
pytest-cov
pytest-html
PyYAML>=6.0.1,<7.0.0
rich>=13.3.1,<14.0.0
rouge-score>=0.1.2,<0.2.0
sentencepiece>=0.2.0,<0.3.0
setuptools>=64
setuptools-scm>=8
tokenizers>=0.15.2,<0.16.0
toml>=0.10.2,<0.11.0
torch>=2.2.1,<3.0.0
tox>=4.4.2,<5
tqdm>=4.66.2,<5.0.0
transformers>=4.30.0,<=4.38.2
trl>=0.7.11,<0.8.0
wandb>=0.16.4,<0.17.0
# the below library should NOT be imported into any python files
# it is for CLI usage ONLY
yamllint>=1.35.1,<1.36.0
